Task 2 - Value Iteration

Answers:


6) 	Rounds of value iteration for start state to become non-zero: 10
    Why? 
    Firstly, in each round of iteration, we only perform one sweep. In order to change the value, we need the sum of dynamics * (reward + discount * next_state_value) to be non-zero.
    In MazeGrid, We initialize all state value to zero, and the only non-zero reward is in terminal state. Therefore, we need the value to propagate from terminal state to initial state, 
	then we can update the corresponding value.
	In this case, this question can be solved by finding the shortest path from terminal state to initial state, which is 10.

7) 	Which parameter to change: noise
	Value of the changed parameter: 0.01

8)	Parameter values producing optimal policy types:
	    a) -n 0.0 -d 0.3
	    b) -n 0.1 -d 0.3
	    c) -n 0.0 -d 0.5
	    d) -n 0.1 -d 0.5
	    e) -n 0.0 -d 0.0

9) 	Pros: 								Cons:
	-									- Requires a full policy evaluation for each updated policy
	-									- Requires iterative computation with typically multiple sweeps through the state set
	-									- Can be computationally expensive

